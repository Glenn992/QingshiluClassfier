# services/data_model.py

import os
import json
import time
import csv
import traceback

# ğŸŒŸã€æ³¨æ„ã€‘æˆ‘ä»¬ä¿æŒ category_structure.py æ–‡ä»¶ä¸å˜ï¼Œå®ƒå¯¼å…¥çš„æ˜¯åŸå§‹ç»“æ„
from gemini.services.category_structure import DEFAULT_CATEGORY_STRUCTURE
from gemini.services.keywords_data import QING_SHILU_KEYWORDS
from gemini.services.constants import CLASSIFIED_DATA_FILE, CUSTOM_KEYWORD_FILE, HISTORY_FILE

# L1 é”®çš„æ˜¾ç¤ºåç§°æ˜ å°„ï¼Œç”¨äºåœ¨ä¸ä¿®æ”¹ category_structure.py çš„å‰æä¸‹ç”Ÿæˆ 'name' å­—æ®µ
# è¿™æ˜¯æ ¹æ®æ‚¨æä¾›çš„ category_structure.py ä¸­çš„æ³¨é‡Šç¡®å®šçš„ã€‚
L1_NAME_MAP = {
    "0": "äº‹åŠ¡ç±» (Affairs)",
    "1": "é—®é¢˜ç±» (Issues)",
}


class DataModel:
    """
    è´Ÿè´£ç®¡ç†å’ŒæŒä¹…åŒ–åº”ç”¨çš„æ‰€æœ‰æ•°æ®ï¼šåˆ†ç±»ç»“æ„ã€å…³é”®è¯ã€å†å²è®°å½•ã€‚
    """

    def __init__(self):
        self.classifiedData = {}
        self.translationHistory = []
        self.customKeywordMap = {}
        # categoryStructure å­˜å‚¨çš„æ˜¯ category_structure.py å¯¼å…¥çš„åŸå§‹ç»“æ„
        self.categoryStructure = self._get_default_category_structure()
        self.qingShiluKeywords = self._get_qing_shilu_keywords()

        self.mergedKeywordMap = {**self.qingShiluKeywords}

        self.load_all_data()

        print(f"DEBUG(Model): Merged Keyword Map size: {len(self.mergedKeywordMap)}")

    def _get_qing_shilu_keywords(self):
        """åŠ è½½ã€Šæ¸…å®å½•ã€‹ä¸“å±è¯åº“"""
        return QING_SHILU_KEYWORDS

    def _get_default_category_structure(self):
        """åŠ è½½é»˜è®¤åˆ†ç±»ç»“æ„"""
        return DEFAULT_CATEGORY_STRUCTURE

    def get_category_structure(self):
        """
        æä¾›ç»™å¤–éƒ¨è·å–å®Œæ•´çš„åˆ†ç±»ç»“æ„ã€‚
        ğŸŒŸã€æ ¸å¿ƒä¿®å¤ã€‘: åŠ¨æ€åœ°ä¸º L1 é”®æ·»åŠ  'name' å­—æ®µå’Œ 'levels' åµŒå¥—ï¼Œä»¥é€‚åº” UI éœ€æ±‚ã€‚
        """
        safe_structure = {}
        for l1_key, l1_data in self.categoryStructure.items():

            l1_name = L1_NAME_MAP.get(l1_key, f"æœªçŸ¥ L1 ({l1_key})")

            # æ£€æŸ¥ L1_data æ˜¯å¦æ˜¯å­—å…¸ (åŒ…å« L2 é”®)
            if isinstance(l1_data, dict) and l1_data:

                # æ„é€ ç¬¦åˆ UI (stats_tab.py) æœŸæœ›çš„ç»“æ„:
                # { "0": { "name": "äº‹åŠ¡ç±»", "levels": { "èµˆç¾ä¸æ°‘ç”Ÿä¿éšœ": {...} } } }
                safe_structure[l1_key] = {
                    'name': l1_name,
                    # å°†åŸå§‹ L1 é”®ä¸‹çš„æ‰€æœ‰å†…å®¹è§†ä¸º L2/L3 çš„ 'levels'
                    'levels': l1_data
                }
            else:
                print(f"è­¦å‘Š: L1 åˆ†ç±»ç»“æ„ä¸­é”® '{l1_key}' çš„æ•°æ®æ ¼å¼é”™è¯¯ï¼Œå·²å¿½ç•¥ã€‚")

        # ğŸŒŸã€è°ƒè¯•ä¿¡æ¯ã€‘æ·»åŠ æ—¥å¿—ï¼Œç¡®è®¤è¿”å›ç»™ UI çš„ç»“æ„æ˜¯å¦åŒ…å« 'name'
        if safe_structure:
            sample_key = next(iter(safe_structure))
            print(
                f"DEBUG(Model): get_category_structure output sample (L1 key '{sample_key}'): {list(safe_structure[sample_key].keys())}")

        return safe_structure

    # =================================================================
    # ä»¥ä¸‹æ–¹æ³•ä¿æŒä¸å˜æˆ–ä»…æœ‰è½»å¾®è°ƒæ•´ä»¥ç¡®ä¿å…¼å®¹æ€§
    # =================================================================

    def load_data_from_json(self, file_path, default_data=None):
        """é€šç”¨ JSON æ–‡ä»¶åŠ è½½å‡½æ•°"""
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½ {file_path}ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®ã€‚é”™è¯¯: {e}")
                return default_data if default_data is not None else {}
        return default_data if default_data is not None else {}

    def save_data_to_json(self, data, file_path):
        """é€šç”¨ JSON æ–‡ä»¶ä¿å­˜å‡½æ•°"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜æ•°æ®åˆ° {file_path}. é”™è¯¯: {e}")

    def load_all_data(self):
        """åŠ è½½æ‰€æœ‰æŒä¹…åŒ–æ•°æ®"""
        self.classifiedData = self.load_data_from_json(CLASSIFIED_DATA_FILE)
        self.translationHistory = self.load_data_from_json(HISTORY_FILE, default_data=[])
        self.customKeywordMap = self.load_data_from_json(CUSTOM_KEYWORD_FILE)
        self._update_merged_keyword_map()

    def _update_merged_keyword_map(self):
        """åˆå¹¶ã€Šæ¸…å®å½•ã€‹è‡ªå¸¦è¯åº“å’Œè‡ªå®šä¹‰è¯åº“"""
        self.mergedKeywordMap = {**self.qingShiluKeywords, **self.customKeywordMap}

    def save_classified_text(self, original_text, translation, classification_key, article_id: str | None = None):
        """ä¿å­˜å·²åˆ†ç±»çš„æ–‡æœ¬ï¼Œæ–°å¢ article_id ç”¨äºæ‰¹é‡å¤„ç†çš„æ ‡è¯†ï¼ˆJS: saveClassificationï¼‰"""

        # ç¤ºä¾‹ key: '0/èµˆç¾ä¸æ°‘ç”Ÿä¿éšœ/èµˆç¾'
        l1, l2, l3 = classification_key.split('/')

        if l1 not in self.classifiedData:
            self.classifiedData[l1] = {}
        if l2 not in self.classifiedData[l1]:
            self.classifiedData[l1][l2] = {}
        if l3 not in self.classifiedData[l1][l2]:
            self.classifiedData[l1][l2][l3] = []

        new_entry = {
            "originalText": original_text,
            "translation": translation,
            "articleId": article_id,
            "timestamp": time.time()
        }

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…·æœ‰ç›¸åŒ articleId çš„æ¡ç›®
        is_updated = False
        if article_id:
            try:
                articles_list = self.classifiedData[l1][l2][l3]
                for i, existing_entry in enumerate(articles_list):
                    if existing_entry.get("articleId") == article_id:
                        articles_list[i] = new_entry
                        is_updated = True
                        break
            except KeyError:
                pass

        if not is_updated:
            self.classifiedData[l1][l2][l3].append(new_entry)

        self.save_data_to_json(self.classifiedData, CLASSIFIED_DATA_FILE)

    def update_custom_keywords(self, category_key, keywords):
        """æ›´æ–°è‡ªå®šä¹‰å…³é”®è¯å¹¶ä¿å­˜ï¼ˆJS: saveKeywordsï¼‰"""
        # key æ ¼å¼: "äº‹åŠ¡ç±»-èµˆç¾ä¸æ°‘ç”Ÿä¿éšœ-èµˆç¾"
        if category_key not in self.customKeywordMap:
            self.customKeywordMap[category_key] = {"keywords": [], "description": "è‡ªå®šä¹‰å…³é”®è¯"}

        self.customKeywordMap[category_key]['keywords'] = keywords

        self._update_merged_keyword_map()
        self.save_data_to_json(self.customKeywordMap, CUSTOM_KEYWORD_FILE)

    def find_category_cases(self, l1, l2, l3):
        """æŸ¥æ‰¾æŒ‡å®šåˆ†ç±»ä¸‹çš„æ¡ˆä¾‹æ–‡æœ¬"""
        return [
            item['originalText']
            for item in self.classifiedData.get(l1, {}).get(l2, {}).get(l3, [])
        ]

    def get_all_classified_data(self):
        """è¿”å›æ‰€æœ‰çš„åˆ†ç±»æ•°æ®ï¼ˆclassifiedDataï¼‰"""
        return self.classifiedData

    # =================================================================
    # ğŸŒŸã€æ–°å¢åŠŸèƒ½ã€‘ç»Ÿè®¡å’Œç­›é€‰æ•°æ®æ–¹æ³• (ä¿æŒå…¼å®¹æ€§)
    # =================================================================

    def get_classified_stats(self, filter_key: str | None = None):
        """
        æ ¹æ® filter_key è¿”å›ç»“æ„åŒ–çš„åˆ†ç±»ç»Ÿè®¡æ•°æ®ã€‚
        æ³¨æ„ï¼šæ­¤æ–¹æ³•æ˜¯åŸºäº self.categoryStructure çš„ L2/L3 é”®åæ¥éå† classifiedData çš„ã€‚
        """

        # è§£æç­›é€‰é”®
        if filter_key:
            parts = filter_key.split('/')
            l1_filter = parts[0] if len(parts) > 0 else None
            l2_filter = parts[1] if len(parts) > 1 else None
            l3_filter = parts[2] if len(parts) > 2 else None
        else:
            l1_filter, l2_filter, l3_filter = None, None, None

        stats_result = {}
        total_count_all = 0

        # éå† classifiedData çš„ L1 é”® ('0', '1')
        for l1_key, l1_data in self.classifiedData.items():
            if l1_filter and l1_key != l1_filter:
                continue

            # ğŸŒŸ ä½¿ç”¨ get_category_structure è·å¾—çš„ç»“æ„æ¥è·å– L1 name å’Œ L2 levels
            l1_cat_structure = self.get_category_structure().get(l1_key)
            if not l1_cat_structure:
                print(f"è­¦å‘Š: classifiedData ä¸­å‘ç°æœªçŸ¥çš„ L1 é”® '{l1_key}'ã€‚è·³è¿‡ç»Ÿè®¡ã€‚")
                continue

            l1_name = l1_cat_structure.get('name', f"æœªçŸ¥ L1 ({l1_key})")
            l1_levels = l1_cat_structure.get('levels', {})  # è·å– L2/L3 åµŒå¥—ç»“æ„

            l1_stats = {
                'name': l1_name,
                'count': 0,
                'levels': {}
            }

            # éå† L2 é”® (è¿™é‡Œä½¿ç”¨ categoryStructure ä¸­çš„ L2 é”®åï¼Œç¡®ä¿å®Œæ•´æ€§)
            for l2_name, l3_map in l1_levels.items():
                if l2_filter and l2_name != l2_filter:
                    continue

                # ä»å®é™…åˆ†ç±»æ•°æ®ä¸­è·å– L2 æ•°æ® (å³ classifiedData[l1_key][l2_name])
                l2_data_actual = l1_data.get(l2_name, {})

                l2_stats = {
                    'count': 0,
                    'levels': {}
                }

                # éå† L3 é”®
                for l3_name in l3_map.keys():
                    if l3_filter and l3_name != l3_filter:
                        continue

                    # ä»å®é™…æ•°æ®ä¸­è·å– L3 æ–‡ç« åˆ—è¡¨
                    articles = l2_data_actual.get(l3_name, [])
                    l3_count = len(articles)

                    # ç´¯åŠ ç»Ÿè®¡
                    l2_stats['count'] += l3_count
                    if l3_filter is None:
                        l2_stats['levels'][l3_name] = l3_count

                # å¦‚æœ L2 æœ‰æ•°æ® (æˆ–è€… L2 æ²¡è¢«ç­›é€‰ä½† L3 æœ‰æ•°æ®)
                if l2_stats['count'] > 0:
                    l1_stats['count'] += l2_stats['count']
                    if l2_filter is None:
                        l1_stats['levels'][l2_name] = l2_stats

            # å¦‚æœ L1 æœ‰æ•°æ®
            if l1_stats['count'] > 0:
                stats_result[l1_key] = l1_stats
                total_count_all += l1_stats['count']

        # è°ƒæ•´ç»“æœç»“æ„ä»¥é€‚åº” StatsTabWidget çš„æ¸²æŸ“ (çœç•¥äº†è¿‡æ»¤ç»†èŠ‚ï¼Œä¿æŒä»£ç å®Œæ•´æ€§)
        if l3_filter:
            final_result = {}
            if l1_filter in stats_result:
                l1_stats = stats_result[l1_filter]

                l2_stats_temp = l1_stats['levels'].get(l2_filter, {'count': 0, 'levels': {}})
                actual_l3_count = len(self.classifiedData.get(l1_filter, {}).get(l2_filter, {}).get(l3_filter, []))

                l2_stats_temp['count'] = actual_l3_count
                l2_stats_temp['levels'] = {l3_filter: actual_l3_count}

                l1_stats['levels'] = {l2_filter: l2_stats_temp}
                l1_stats['count'] = actual_l3_count
                final_result[l1_filter] = l1_stats
            return final_result

        return stats_result

    def _get_filtered_articles(self, filter_key: str | None = None):
        """æ ¹æ® filter_key è·å–æ‰€æœ‰åŒ¹é…çš„æ¡ç›®åˆ—è¡¨ (ç”¨äºå¯¼å‡º)"""

        articles_to_export = []

        # è§£æç­›é€‰é”®
        if filter_key:
            parts = filter_key.split('/')
            l1_filter = parts[0] if len(parts) > 0 else None
            l2_filter = parts[1] if len(parts) > 1 else None
            l3_filter = parts[2] if len(parts) > 2 else None
        else:
            l1_filter, l2_filter, l3_filter = None, None, None

        for l1_key, l1_data in self.classifiedData.items():
            if l1_filter and l1_key != l1_filter:
                continue

            for l2_name, l2_data in l1_data.items():
                if l2_filter and l2_name != l2_filter:
                    continue

                for l3_name, articles in l2_data.items():
                    if l3_filter and l3_name != l3_filter:
                        continue

                    for article in articles:
                        # æ„é€ å®Œæ•´è¡Œæ•°æ®
                        articles_to_export.append({
                            "Level1": l1_key,
                            "Level2": l2_name,
                            "Level3": l3_name,
                            "OriginalText": article['originalText'],
                            "Translation": article.get('translation', 'N/A'),
                            "ArticleId": article.get('articleId', 'N/A'),
                            "Timestamp": article['timestamp']
                        })

        return articles_to_export

    def export_classified_data_to_csv(self, file_path, filter_key: str | None = None):
        """å°†æ‰€æœ‰åˆ†ç±»æ•°æ®å¯¼å‡ºä¸º CSV æ ¼å¼ï¼Œå¯æ ¹æ® filter_key ç­›é€‰"""

        articles_to_export = self._get_filtered_articles(filter_key)

        if not articles_to_export:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ•°æ®å¯ä»¥å¯¼å‡ºã€‚")
            try:
                with open(file_path, 'w', encoding='utf-8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(
                        ["Level1", "Level2", "Level3", "OriginalText", "Translation", "ArticleId", "Timestamp"])
                return True
            except Exception as e:
                print(f"Error creating empty CSV file: {e}")
                return False

        header = ["Level1", "Level2", "Level3", "OriginalText", "Translation", "ArticleId", "Timestamp"]

        try:
            with open(file_path, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(header)

                for article in articles_to_export:
                    # æ ¼å¼åŒ–æ—¶é—´æˆ³
                    formatted_time = time.strftime(
                        '%Y-%m-%d %H:%M:%S',
                        time.localtime(article['Timestamp'])
                    )

                    writer.writerow([
                        article['Level1'],
                        article['Level2'],
                        article['Level3'],
                        article['OriginalText'].replace('\n', ' ').strip(),
                        article['Translation'].replace('\n', ' ').strip(),
                        article['ArticleId'],
                        formatted_time
                    ])
            return True
        except Exception as e:
            print(f"Error exporting CSV: {e}")
            print(traceback.format_exc())
            return False
